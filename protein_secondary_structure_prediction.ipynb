{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fb82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.4099, Train Accuracy: 0.8374\n",
      "Epoch 2/10, Train Loss: 0.3587, Train Accuracy: 0.8587\n",
      "Epoch 3/10, Train Loss: 0.3456, Train Accuracy: 0.8640\n",
      "Epoch 4/10, Train Loss: 0.3361, Train Accuracy: 0.8687\n",
      "Epoch 5/10, Train Loss: 0.3317, Train Accuracy: 0.8699\n",
      "Epoch 6/10, Train Loss: 0.3287, Train Accuracy: 0.8715\n",
      "Epoch 7/10, Train Loss: 0.3245, Train Accuracy: 0.8731\n",
      "Epoch 8/10, Train Loss: 0.3194, Train Accuracy: 0.8750\n",
      "Epoch 9/10, Train Loss: 0.3160, Train Accuracy: 0.8764\n",
      "Epoch 10/10, Train Loss: 0.3162, Train Accuracy: 0.8764\n",
      "Q3 Accuracy on Validation Set: 0.8705\n",
      "Submission file saved to /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Mapping from amino acid characters to integers\n",
    "aa_map = {\n",
    "    'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
    "    'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
    "    'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
    "    'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19,\n",
    "    'X': 20,  'B': 21,  'Z': 22,  'J': 23,  '-': 24,\n",
    "}\n",
    "\n",
    "# Mapping for secondary structure labels\n",
    "ss_map = {'H': 0, 'E': 1, 'C': 2}\n",
    "\n",
    "# Custom dataset class for handling protein data\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, seqs_csv_file, train_directory, labels_csv_file=None, normalization_method='min-max'):\n",
    "        # Reading the sequence feature file\n",
    "        self.seqs_data = pd.read_csv(seqs_csv_file)\n",
    "        self.protein_data = {}\n",
    "        \n",
    "        # Getting residue number, amino acid and PSSM profiles for every protein\n",
    "        for filename in os.listdir(train_directory):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                # Given the folder name, checks all folders that terminate with either _train or _test\n",
    "                pdb_id = re.split(r'_train|_test', filename)[0] \n",
    "                self.protein_data[pdb_id] = pd.read_csv(os.path.join(train_directory, filename))\n",
    "                \n",
    "        # Getting the ground labels (Secondary Structure)\n",
    "        if labels_csv_file:\n",
    "            self.labels_data = pd.read_csv(labels_csv_file)\n",
    "        else:\n",
    "            self.labels_data = None\n",
    "        self.aa_mapping = aa_map\n",
    "        \n",
    "        # Setting the normalization method (default -> min-max)\n",
    "        self.normalization_method = normalization_method\n",
    "\n",
    "    def get_seq_encoding(self, sequence):\n",
    "        # Function to encode the sequence based on the amino acid mapping\n",
    "        seq_len = len(sequence)\n",
    "        aa_map_len = len(self.aa_mapping)\n",
    "        encoded_seq = np.zeros((seq_len, aa_map_len), dtype=int)\n",
    "        \n",
    "        # Maps the corresponding value of Amino Acid - X for unknown amino acids\n",
    "        for i, aa in enumerate(sequence):\n",
    "            index = self.aa_mapping.get(aa, self.aa_mapping['X'])\n",
    "            encoded_seq[i, index] = 1\n",
    "        return encoded_seq\n",
    "\n",
    "    def get_norm_pssm(self, pssm):\n",
    "        # Function to normalize the PSSM Profile\n",
    "        numeric_columns = pssm[:, 2:]\n",
    "        pssm_num = numeric_columns.astype(np.float32)\n",
    "\n",
    "        if self.normalization_method == 'min-max':\n",
    "            # Normalizing the PSSM profile using min-max algorithm\n",
    "            pssm_min = pssm_num.min(axis=0)\n",
    "            pssm_max = pssm_num.max(axis=0)\n",
    "            pssm_range = np.where(pssm_max - pssm_min == 0, 1, \n",
    "                                  pssm_max - pssm_min)\n",
    "            normalized_pssm = (pssm_num - pssm_min) / pssm_range\n",
    "        elif self.normalization_method == 'z-score':\n",
    "            # Normalizing the PSSM profile using z-score algorithm\n",
    "            pssm_mean = pssm_numeric.mean(axis=0)\n",
    "            pssm_std = pssm_numeric.std(axis=0)\n",
    "            pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "            normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "        else:\n",
    "            normalized_pssm = pssm_numeric\n",
    "\n",
    "        return normalized_pssm\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Getting the PDB ID, Sequence \n",
    "        pdb_id = self.seqs_data.iloc[idx]['PDB_ID']\n",
    "        seqs_data = self.seqs_data.iloc[idx]['SEQUENCE']\n",
    "        \n",
    "        # Encoding the sequence\n",
    "        encoded_seq = self.get_seq_encoding(seqs_data)\n",
    "        pssm = self.protein_data[pdb_id].values\n",
    "        norm_pssm = self.get_norm_pssm(pssm)\n",
    "                               \n",
    "        encoded_seq_tensor = torch.tensor(encoded_seq, dtype=torch.float32)\n",
    "        norm_pssm_tensor = torch.tensor(norm_pssm, dtype=torch.float32)\n",
    "\n",
    "        # Getting the ground truth labels (secondary structure)\n",
    "        if self.labels_data is not None:\n",
    "            label_seq = self.labels_data.iloc[idx]['SEC_STRUCT']\n",
    "            \n",
    "            # Encoding the labels using secondarys structure map \n",
    "            label_num = [ss_map[char] for char in label_seq]\n",
    "            \n",
    "            # Converting the encoded labels to pytorch tensors\n",
    "            label_tensor = torch.tensor(label_num, dtype=torch.long)\n",
    "                    \n",
    "            return (pdb_id, encoded_seq_tensor, norm_pssm_tensor, label_tensor)\n",
    "\n",
    "        return (pdb_id, encoded_seq_tensor, norm_pssm_tensor)\n",
    "    \n",
    "def custom_collate_fn_nolabel(batch):\n",
    "    # Collate function for DataLoader that doesn't have labels\n",
    "    ids, sequences, pssms = zip(*batch)\n",
    "    \n",
    "    # Padding the sequences to match the max sequence length\n",
    "    pad_seq_list = [seq.clone().detach() for seq in sequences]\n",
    "    sequences_padded = pad_sequence(pad_seq_list, batch_first=True)\n",
    "    \n",
    "    # Padding the pssms\n",
    "    pssms_padded = torch.tensor(pssms)\n",
    "    return ids, sequences_padded, pssms_padded\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Collate function for DataLoader with labels\n",
    "    _, sequences, pssms, labels_list = zip(*batch)\n",
    "    \n",
    "    # Padding the sequences to match the max sequence length\n",
    "    pad_seq_list = [seq.clone().detach() for seq in sequences]\n",
    "    sequences_padded = pad_sequence(pad_seq_list, batch_first=True)\n",
    "    \n",
    "    # Padding the pssms\n",
    "    pd_pssms_list = [pssm.clone().detach() for pssm in pssms]\n",
    "    pssms_padded = pad_sequence(pd_pssms_list, batch_first=True)\n",
    "    \n",
    "    # Padding the secondary structures to match the max length\n",
    "    if labels_list[0] is not None:\n",
    "        pad_labels_lst = [label.clone().detach() for label in labels_list]\n",
    "        labels_padded = pad_sequence(pad_labels_lst, batch_first=True)\n",
    "    else:\n",
    "        labels_padded = None\n",
    "        \n",
    "    # Adding a mask for the padded labels\n",
    "    mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "    mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return sequences_padded, pssms_padded, labels_padded, mask_padded\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    # Class to construct a Fully Convolutional Protein Model using PyTorch\n",
    "    def __init__(self, num_classes=3, input_channels=20):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        \n",
    "        # Creating 3 1D CNNs\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.final_conv = nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward method for the CNN network\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Final conv layer\n",
    "        x = self.final_conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "# Function to train the model\n",
    "def model_train(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    # If Nvidia CUDA is available, use that otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Moving the model to GPU\n",
    "    model.to(device)\n",
    "    \n",
    "    # Train the network over multiple epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training the model\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for sequences, pssms, labels, _ in train_loader:\n",
    "            \n",
    "            # Getting the input pssms\n",
    "            inputs = pssms.permute(0, 2, 1).to(device)\n",
    "\n",
    "            # Initializing optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Model outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Moving the labels to GPU\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Calculating the loss\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "\n",
    "            # Back Propagation to update the weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculating the cumulative loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Getting the predictions\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "        # Calculating the loss per epoch\n",
    "        td = train_loader.dataset\n",
    "        epoch_loss = running_loss / len(td)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}')\n",
    "        \n",
    "# Main part of the script\n",
    "seqs_csv_dir = '/kaggle/input/deep-learning-for-msc-202324/seqs_train.csv'\n",
    "labels_csv_dir = '/kaggle/input/deep-learning-for-msc-202324/labels_train.csv'\n",
    "train_path = '/kaggle/input/deep-learning-for-msc-202324/train'\n",
    "test_path = '/kaggle/input/deep-learning-for-msc-202324/test'\n",
    "\n",
    "# Load data\n",
    "dataset = ProteinDataset(seqs_csv_file=seqs_csv_dir, train_directory=train_path, labels_csv_file=labels_csv_dir)\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Validation DataLoader\n",
    "val_loader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Training the model\n",
    "model = ProteinModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# According to Ax, these are best hyperparameters, however, these don't give good accuracy\n",
    "# Best Parameters: {'lr': 0.00010943465377382328, 'weight_decay': 0.0, 'num_epochs': 9}\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "# Best num epochs\n",
    "num_epochs = 10\n",
    "\n",
    "model_train(model, criterion, optimizer, train_loader, num_epochs)\n",
    "\n",
    "def calculate_q3_accuracy(predictions, ground_truth):\n",
    "    # Function to calculate Q3 Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        for p, t in zip(pred, truth):\n",
    "            if p == t:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    q3_accuracy = correct / total\n",
    "    return q3_accuracy\n",
    "\n",
    "# Test model and calculate Q3 accuracy on Validation Set\n",
    "def test_model_and_calc_q3(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, pssms, labels, _ in val_loader:\n",
    "            inputs = pssms.permute(0, 2, 1).to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "\n",
    "            predicted = predicted.view(-1)\n",
    "\n",
    "            for pred, label in zip(predicted, labels.view(-1)):\n",
    "                pred_index = pred.item()\n",
    "                structure_label = ['H', 'E', 'C'][pred_index]\n",
    "                predictions.append(structure_label)\n",
    "\n",
    "                gt_label = ['H', 'E', 'C'][label.item()]\n",
    "                ground_truth.append(gt_label)\n",
    "\n",
    "    q3_accuracy = calculate_q3_accuracy(predictions, ground_truth)\n",
    "    print(f'Q3 Accuracy on Validation Set: {q3_accuracy:.4f}')\n",
    "    return q3_accuracy\n",
    "\n",
    "q3_accuracy = test_model_and_calc_q3(model, val_loader)\n",
    "\n",
    "def test(model, test_dataset, output_file='/kaggle/working/submission.csv'):\n",
    "    # If Nvidia CUDA is available, use that otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Evaluate the model (no training - so backprop won't be performed)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterating through the test dataset\n",
    "        for i in range(len(test_dataset)):\n",
    "            \n",
    "            # Getting the PDB ID and PSSM values\n",
    "            pdb_id, _, pssm = test_dataset[i]\n",
    "            input_pssm = pssm.unsqueeze(0).permute(0, 2, 1).to(device)\n",
    "\n",
    "            # Getting outputs from the model\n",
    "            outputs = model(input_pssm)\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "\n",
    "            seq_len = pssm.shape[0]\n",
    "            # Iterating through the sequences\n",
    "            for j in range(seq_len):\n",
    "                # Forming predictions with the format PDB_ID_RESIDUE_NUMBER, SECONDARY_STRUCTURE\n",
    "                residue_id = f\"{pdb_id}_{j+1}\"\n",
    "                struct_label = ['H', 'E', 'C'][predicted[0, j].item()]\n",
    "                predictions.append([residue_id, struct_label])\n",
    "\n",
    "    # Saving the predictions to submission.csv\n",
    "    submission_df = pd.DataFrame(predictions, columns=['ID', 'STRUCTURE'])\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f'Submission file saved to {output_file}')\n",
    "\n",
    "seqs_test_dir = '/kaggle/input/deep-learning-for-msc-202324/seqs_test.csv'\n",
    "test_path = '/kaggle/input/deep-learning-for-msc-202324/test'\n",
    "\n",
    "test_dataset = ProteinDataset(seqs_csv_file=seqs_test_dir, train_directory=test_path)\n",
    "\n",
    "test(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca40ab",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96af6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ax-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afa8e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:25:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter lr. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-18 14:25:13] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter weight_decay. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-18 14:25:13] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter num_epochs. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 03-18 14:25:13] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='lr', parameter_type=FLOAT, range=[1e-05, 0.1], log_scale=True), RangeParameter(name='weight_decay', parameter_type=FLOAT, range=[0.0, 0.1]), RangeParameter(name='num_epochs', parameter_type=INT, range=[9, 10])], parameter_constraints=[]).\n",
      "[INFO 03-18 14:25:13] ax.modelbridge.dispatch_utils: Using Models.GPEI since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 03-18 14:25:13] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=3 num_trials=None use_batch_trials=False\n",
      "[INFO 03-18 14:25:13] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=6\n",
      "[INFO 03-18 14:25:13] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=6\n",
      "[INFO 03-18 14:25:13] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 6 trials, GPEI for subsequent trials]). Iterations after 6 will take longer to generate due to model-fitting.\n",
      "[INFO 03-18 14:25:13] ax.service.managed_loop: Started full optimization with 10 steps.\n",
      "[INFO 03-18 14:25:13] ax.service.managed_loop: Running optimization trial 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameterization: {'lr': 0.08265406608089564, 'weight_decay': 0.09397713541984559, 'num_epochs': 9}\n",
      "Epoch 1/9, Loss: 251.5962, Accuracy: 0.6389\n",
      "Epoch 2/9, Loss: 3.3524, Accuracy: 0.6308\n",
      "Epoch 3/9, Loss: 20.4914, Accuracy: 0.6362\n",
      "Epoch 4/9, Loss: 0.9169, Accuracy: 0.6422\n",
      "Epoch 5/9, Loss: 0.9168, Accuracy: 0.6423\n",
      "Epoch 6/9, Loss: 0.9128, Accuracy: 0.6466\n",
      "Epoch 7/9, Loss: 0.9144, Accuracy: 0.6452\n",
      "Epoch 8/9, Loss: 0.9114, Accuracy: 0.6462\n",
      "Epoch 9/9, Loss: 0.9141, Accuracy: 0.6452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:26:39] ax.service.managed_loop: Running optimization trial 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.6388\n",
      "q3_accuracy: 0.6387708592178695\n",
      "Parameterization: {'lr': 0.0003738899150858358, 'weight_decay': 0.07317882720381022, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.6596, Accuracy: 0.6991\n",
      "Epoch 2/10, Loss: 0.6303, Accuracy: 0.7273\n",
      "Epoch 3/10, Loss: 0.6248, Accuracy: 0.7291\n",
      "Epoch 4/10, Loss: 0.6282, Accuracy: 0.7283\n",
      "Epoch 5/10, Loss: 0.6259, Accuracy: 0.7285\n",
      "Epoch 6/10, Loss: 0.6220, Accuracy: 0.7310\n",
      "Epoch 7/10, Loss: 0.6263, Accuracy: 0.7287\n",
      "Epoch 8/10, Loss: 0.6253, Accuracy: 0.7292\n",
      "Epoch 9/10, Loss: 0.6273, Accuracy: 0.7296\n",
      "Epoch 10/10, Loss: 0.6217, Accuracy: 0.7318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:28:02] ax.service.managed_loop: Running optimization trial 3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.7225\n",
      "q3_accuracy: 0.7224891713973874\n",
      "Parameterization: {'lr': 1.7980909214052284e-05, 'weight_decay': 0.013840343151241542, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.9017, Accuracy: 0.6462\n",
      "Epoch 2/10, Loss: 0.6230, Accuracy: 0.7110\n",
      "Epoch 3/10, Loss: 0.5674, Accuracy: 0.7579\n",
      "Epoch 4/10, Loss: 0.5458, Accuracy: 0.7728\n",
      "Epoch 5/10, Loss: 0.5404, Accuracy: 0.7755\n",
      "Epoch 6/10, Loss: 0.5370, Accuracy: 0.7773\n",
      "Epoch 7/10, Loss: 0.5325, Accuracy: 0.7784\n",
      "Epoch 8/10, Loss: 0.5235, Accuracy: 0.7816\n",
      "Epoch 9/10, Loss: 0.5172, Accuracy: 0.7830\n",
      "Epoch 10/10, Loss: 0.5137, Accuracy: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:29:26] ax.service.managed_loop: Running optimization trial 4...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.7810\n",
      "q3_accuracy: 0.7810185407642831\n",
      "Parameterization: {'lr': 2.0776441211156205e-05, 'weight_decay': 0.07524346448481084, 'num_epochs': 9}\n",
      "Epoch 1/9, Loss: 0.9667, Accuracy: 0.6466\n",
      "Epoch 2/9, Loss: 0.8797, Accuracy: 0.6467\n",
      "Epoch 3/9, Loss: 0.7616, Accuracy: 0.6476\n",
      "Epoch 4/9, Loss: 0.6791, Accuracy: 0.6436\n",
      "Epoch 5/9, Loss: 0.6643, Accuracy: 0.6460\n",
      "Epoch 6/9, Loss: 0.6567, Accuracy: 0.6762\n",
      "Epoch 7/9, Loss: 0.6521, Accuracy: 0.6950\n",
      "Epoch 8/9, Loss: 0.6500, Accuracy: 0.7014\n",
      "Epoch 9/9, Loss: 0.6449, Accuracy: 0.7062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:30:45] ax.service.managed_loop: Running optimization trial 5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.7040\n",
      "q3_accuracy: 0.7039844452529191\n",
      "Parameterization: {'lr': 0.0004038132555993809, 'weight_decay': 0.013856762927025557, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.5105, Accuracy: 0.7886\n",
      "Epoch 2/10, Loss: 0.4603, Accuracy: 0.8176\n",
      "Epoch 3/10, Loss: 0.4559, Accuracy: 0.8196\n",
      "Epoch 4/10, Loss: 0.4542, Accuracy: 0.8204\n",
      "Epoch 5/10, Loss: 0.4515, Accuracy: 0.8209\n",
      "Epoch 6/10, Loss: 0.4491, Accuracy: 0.8217\n",
      "Epoch 7/10, Loss: 0.4517, Accuracy: 0.8218\n",
      "Epoch 8/10, Loss: 0.4502, Accuracy: 0.8215\n",
      "Epoch 9/10, Loss: 0.4489, Accuracy: 0.8225\n",
      "Epoch 10/10, Loss: 0.4465, Accuracy: 0.8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:32:41] ax.service.managed_loop: Running optimization trial 6...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.8212\n",
      "q3_accuracy: 0.8212165102725856\n",
      "Parameterization: {'lr': 2.3520360595684207e-05, 'weight_decay': 0.07029504012316466, 'num_epochs': 9}\n",
      "Epoch 1/9, Loss: 0.9801, Accuracy: 0.6467\n",
      "Epoch 2/9, Loss: 0.8389, Accuracy: 0.6436\n",
      "Epoch 3/9, Loss: 0.6898, Accuracy: 0.6409\n",
      "Epoch 4/9, Loss: 0.6552, Accuracy: 0.6852\n",
      "Epoch 5/9, Loss: 0.6441, Accuracy: 0.7094\n",
      "Epoch 6/9, Loss: 0.6338, Accuracy: 0.7198\n",
      "Epoch 7/9, Loss: 0.6309, Accuracy: 0.7242\n",
      "Epoch 8/9, Loss: 0.6284, Accuracy: 0.7247\n",
      "Epoch 9/9, Loss: 0.6263, Accuracy: 0.7268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:34:02] ax.service.managed_loop: Running optimization trial 7...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.7212\n",
      "q3_accuracy: 0.7211506119619163\n",
      "Parameterization: {'lr': 0.0005185112785196033, 'weight_decay': 0.018613750933033368, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.5434, Accuracy: 0.7729\n",
      "Epoch 2/10, Loss: 0.4998, Accuracy: 0.7912\n",
      "Epoch 3/10, Loss: 0.4849, Accuracy: 0.8066\n",
      "Epoch 4/10, Loss: 0.4736, Accuracy: 0.8130\n",
      "Epoch 5/10, Loss: 0.4708, Accuracy: 0.8143\n",
      "Epoch 6/10, Loss: 0.4746, Accuracy: 0.8137\n",
      "Epoch 7/10, Loss: 0.4701, Accuracy: 0.8144\n",
      "Epoch 8/10, Loss: 0.4732, Accuracy: 0.8140\n",
      "Epoch 9/10, Loss: 0.4699, Accuracy: 0.8140\n",
      "Epoch 10/10, Loss: 0.4690, Accuracy: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:35:31] ax.service.managed_loop: Running optimization trial 8...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.7873\n",
      "q3_accuracy: 0.7872668675649888\n",
      "Parameterization: {'lr': 0.00035406362860655825, 'weight_decay': 0.004160757129606094, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.4631, Accuracy: 0.8134\n",
      "Epoch 2/10, Loss: 0.4157, Accuracy: 0.8343\n",
      "Epoch 3/10, Loss: 0.4177, Accuracy: 0.8343\n",
      "Epoch 4/10, Loss: 0.4120, Accuracy: 0.8370\n",
      "Epoch 5/10, Loss: 0.4147, Accuracy: 0.8365\n",
      "Epoch 6/10, Loss: 0.4083, Accuracy: 0.8385\n",
      "Epoch 7/10, Loss: 0.4108, Accuracy: 0.8378\n",
      "Epoch 8/10, Loss: 0.4057, Accuracy: 0.8390\n",
      "Epoch 9/10, Loss: 0.4096, Accuracy: 0.8388\n",
      "Epoch 10/10, Loss: 0.4077, Accuracy: 0.8389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:36:52] ax.service.managed_loop: Running optimization trial 9...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.8358\n",
      "q3_accuracy: 0.8357501767584895\n",
      "Parameterization: {'lr': 0.00024200160205771219, 'weight_decay': 0.006901753888666292, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.4862, Accuracy: 0.8004\n",
      "Epoch 2/10, Loss: 0.4382, Accuracy: 0.8260\n",
      "Epoch 3/10, Loss: 0.4287, Accuracy: 0.8298\n",
      "Epoch 4/10, Loss: 0.4240, Accuracy: 0.8324\n",
      "Epoch 5/10, Loss: 0.4248, Accuracy: 0.8324\n",
      "Epoch 6/10, Loss: 0.4186, Accuracy: 0.8341\n",
      "Epoch 7/10, Loss: 0.4226, Accuracy: 0.8324\n",
      "Epoch 8/10, Loss: 0.4273, Accuracy: 0.8315\n",
      "Epoch 9/10, Loss: 0.4184, Accuracy: 0.8338\n",
      "Epoch 10/10, Loss: 0.4194, Accuracy: 0.8344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 03-18 14:38:33] ax.service.managed_loop: Running optimization trial 10...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Accuracy on Validation Set: 0.8331\n",
      "q3_accuracy: 0.8330885028041104\n",
      "Parameterization: {'lr': 0.001492506197790241, 'weight_decay': 0.005777916503364055, 'num_epochs': 10}\n",
      "Epoch 1/10, Loss: 0.4533, Accuracy: 0.8188\n",
      "Epoch 2/10, Loss: 0.4307, Accuracy: 0.8289\n",
      "Epoch 3/10, Loss: 0.4274, Accuracy: 0.8310\n",
      "Epoch 4/10, Loss: 0.4260, Accuracy: 0.8315\n",
      "Epoch 5/10, Loss: 0.4232, Accuracy: 0.8318\n",
      "Epoch 6/10, Loss: 0.4212, Accuracy: 0.8334\n",
      "Epoch 7/10, Loss: 0.4221, Accuracy: 0.8323\n",
      "Epoch 8/10, Loss: 0.4217, Accuracy: 0.8327\n",
      "Epoch 9/10, Loss: 0.4231, Accuracy: 0.8327\n",
      "Epoch 10/10, Loss: 0.4187, Accuracy: 0.8338\n",
      "Q3 Accuracy on Validation Set: 0.8339\n",
      "q3_accuracy: 0.8339036511782755\n",
      "Best Parameters: {'lr': 0.00035406362860655825, 'weight_decay': 0.004160757129606094, 'num_epochs': 10}\n",
      "Objective Values: ({'q3_accuracy': 0.8357496082954966}, {'q3_accuracy': {'q3_accuracy': 4.622222494204315e-09}})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from ax import optimize\n",
    "\n",
    "def train_evaluate(parameterization):\n",
    "    print(\"Parameterization:\", parameterization)\n",
    "    lr = parameterization.get('lr', 0.001)\n",
    "    weight_decay = parameterization.get('weight_decay', 0.0)\n",
    "    num_epochs = parameterization.get('num_epochs', 10)\n",
    "\n",
    "    model = ProteinModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_model(model, criterion, optimizer, train_loader, num_epochs)\n",
    "\n",
    "    q3_accuracy = test_model_and_calc_q3(model, val_loader)  # Calculate Q3 accuracy\n",
    "    print(\"q3_accuracy:\", q3_accuracy)\n",
    "\n",
    "    return {\"q3_accuracy\": (q3_accuracy, 0.0)}  # Returning as a dictionary\n",
    "\n",
    "search_space = [\n",
    "    {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 1e-1], \"log_scale\": True},\n",
    "    {\"name\": \"weight_decay\", \"type\": \"range\", \"bounds\": [0.0, 0.1]},\n",
    "    {\"name\": \"num_epochs\", \"type\": \"range\", \"bounds\": [9, 10]},\n",
    "]\n",
    "\n",
    "# seqs_csv_dir = '/kaggle/input/deep-learning-for-msc-202324/seqs_train.csv'\n",
    "# labels_csv_dir = '/kaggle/input/deep-learning-for-msc-202324/labels_train.csv'\n",
    "# train_path = '/kaggle/input/deep-learning-for-msc-202324/train'\n",
    "# test_path = '/kaggle/input/deep-learning-for-msc-202324/test'\n",
    "\n",
    "seqs_csv_dir = '/Users/dhanyasasikumar/Desktop/Computing Science PGT Course/Semester 2/Deep Learning/Coursework/dataset/deep-learning-for-msc-202324/seqs_train.csv'\n",
    "labels_csv_dir = '/Users/dhanyasasikumar/Desktop/Computing Science PGT Course/Semester 2/Deep Learning/Coursework/dataset/deep-learning-for-msc-202324/labels_train.csv'\n",
    "train_path = '/Users/dhanyasasikumar/Desktop/Computing Science PGT Course/Semester 2/Deep Learning/Coursework/dataset/deep-learning-for-msc-202324/train'\n",
    "test_path = '/Users/dhanyasasikumar/Desktop/Computing Science PGT Course/Semester 2/Deep Learning/Coursework/dataset/deep-learning-for-msc-202324/test'\n",
    "\n",
    "\n",
    "dataset = ProteinDataset(seqs_csv_file=seqs_csv_dir, train_directory=train_path, labels_csv_file=labels_csv_dir)\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(test_data, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=search_space,\n",
    "    evaluation_function=train_evaluate,\n",
    "    objective_name='q3_accuracy',\n",
    "    total_trials=10,\n",
    ")\n",
    "\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Objective Values:\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186408e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ad0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7709659,
     "sourceId": 68978,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ocr_env1",
   "language": "python",
   "name": "ocr_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 596.061708,
   "end_time": "2024-03-17T05:52:36.228290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-17T05:42:40.166582",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
